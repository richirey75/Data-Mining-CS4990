{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0LQx8laJoMXQPqcJje29S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/richirey75/Data-Mining-CS4990/blob/main/CS4990GroupAssgn3DBScanModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DBScan From scratch"
      ],
      "metadata": {
        "id": "jzV2f8n8eMgN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V7M3bafVP7PD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def dbscan(data, columns, eps, min_samples):\n",
        "    \"\"\"\n",
        "    Perform DBSCAN clustering on the specified columns of a dataset.\n",
        "\n",
        "    Parameters:\n",
        "      data        : Iterable containing data instances.\n",
        "      columns     : List of column indices to use for clustering.\n",
        "      eps         : Radius to consider neighbors.\n",
        "      min_samples : Minimum number of neighbors to be a core point.\n",
        "\n",
        "    Returns:\n",
        "      List of clusters (each cluster is a list of data instances) and a list of noise points.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a data subset based on specified columns for clustering\n",
        "    # Changed line: Use advanced indexing with columns list\n",
        "    D = np.array([instance for instance in data])\n",
        "    D = D[:, columns]\n",
        "    labels = [0] * len(D)  # Initialize all labels to 0 (unvisited)\n",
        "    C = 0  # Cluster ID\n",
        "\n",
        "    for P in range(len(D)):\n",
        "        if labels[P] != 0:  # Skip if the point has already been processed\n",
        "            continue\n",
        "\n",
        "        NeighborPts = region_query(D, P, eps)\n",
        "\n",
        "        # Check if the point is a core point\n",
        "        if len(NeighborPts) < min_samples:\n",
        "            labels[P] = -1  # Mark as noise\n",
        "        else:\n",
        "            C += 1  # Increment cluster ID\n",
        "            grow_cluster(D, labels, P, NeighborPts, C, eps, min_samples)\n",
        "\n",
        "    # Organize points into clusters and noise\n",
        "    clusters = [[] for _ in range(C)]\n",
        "    noise = []\n",
        "    for i, label in enumerate(labels):\n",
        "        if label == -1:\n",
        "            noise.append(data[i])\n",
        "        else:\n",
        "            clusters[label - 1].append(data[i])\n",
        "\n",
        "    return clusters, noise\n",
        "def grow_cluster(D, labels, P, NeighborPts, C, eps, min_samples):\n",
        "    \"\"\"\n",
        "    Grow a cluster from the seed point `P`.\n",
        "\n",
        "    Parameters:\n",
        "      D           : Data subset with relevant columns\n",
        "      labels      : List of point labels\n",
        "      P           : Seed point index\n",
        "      NeighborPts : List of neighbors of the seed point\n",
        "      C           : Cluster ID\n",
        "      eps         : Radius to consider neighbors\n",
        "      min_samples : Minimum number of neighbors to be a core point\n",
        "    \"\"\"\n",
        "    labels[P] = C  # Assign the cluster label to the seed point\n",
        "\n",
        "    i = 0\n",
        "    while i < len(NeighborPts):\n",
        "        Pn = NeighborPts[i]\n",
        "\n",
        "        if labels[Pn] == -1:\n",
        "            labels[Pn] = C  # Convert noise to border point\n",
        "\n",
        "        elif labels[Pn] == 0:\n",
        "            labels[Pn] = C\n",
        "            PnNeighborPts = region_query(D, Pn, eps)\n",
        "            if len(PnNeighborPts) >= min_samples:\n",
        "                NeighborPts += PnNeighborPts\n",
        "\n",
        "        i += 1\n",
        "\n",
        "\n",
        "def region_query(D, P, eps):\n",
        "    \"\"\"\n",
        "    Find neighbors within `eps` distance from point `P`.\n",
        "\n",
        "    Parameters:\n",
        "      D   : Data subset with relevant columns\n",
        "      P   : Index of the point in question\n",
        "      eps : Radius to consider neighbors\n",
        "\n",
        "    Returns:\n",
        "      List of indices of points within `eps` distance of `P`.\n",
        "    \"\"\"\n",
        "    neighbors = []\n",
        "    for Pn in range(len(D)):\n",
        "        if np.linalg.norm(D[P] - D[Pn]) < eps:\n",
        "            neighbors.append(Pn)\n",
        "\n",
        "    return neighbors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Comparision with sklearn's DBScan"
      ],
      "metadata": {
        "id": "1lVQbElwekpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import DBSCAN as sklearn_DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "def test_dbscan():\n",
        "    \"\"\"\n",
        "    Tests the DBSCAN implementation against scikit-learn's DBSCAN.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load your dataset\n",
        "    data = pd.read_csv(\"combined_tracks_info_ArijitSingh_KK.csv\")  # Replace with your file name\n",
        "\n",
        "    # Select the relevant numerical columns for clustering\n",
        "    # You may need to adjust these columns based on your dataset.\n",
        "    columns_to_cluster = [\n",
        "        \"danceability\", \"energy\", \"loudness\", \"speechiness\", \"acousticness\",\n",
        "        \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"\n",
        "    ]\n",
        "\n",
        "    X = data[columns_to_cluster].values\n",
        "\n",
        "    # Scale the data for better performance\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # DBSCAN parameters\n",
        "    eps = 0.5\n",
        "    min_samples = 5\n",
        "\n",
        "    # Run your DBSCAN implementation\n",
        "    clusters_custom, noise_custom = dbscan(\n",
        "        data=[list(row) for row in X_scaled],\n",
        "        columns=list(range(len(columns_to_cluster))),\n",
        "        eps=eps,\n",
        "        min_samples=min_samples\n",
        "    )\n",
        "\n",
        "    # Run scikit-learn's DBSCAN\n",
        "    dbscan_sklearn = sklearn_DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    labels_sklearn = dbscan_sklearn.fit_predict(X_scaled)\n",
        "\n",
        "    # Compare the results\n",
        "    clusters_sklearn = [[] for _ in range(len(set(labels_sklearn)) - (1 if -1 in labels_sklearn else 0))]\n",
        "    noise_sklearn = []\n",
        "    for i, label in enumerate(labels_sklearn):\n",
        "      if label == -1:\n",
        "        noise_sklearn.append(X_scaled[i])\n",
        "      else:\n",
        "        clusters_sklearn[label].append(X_scaled[i])\n",
        "\n",
        "\n",
        "    # You can implement more detailed comparison here\n",
        "    # For example, compare the number of clusters, noise points, etc.\n",
        "    print(\"Number of clusters (custom):\", len(clusters_custom))\n",
        "    print(\"Number of noise points (custom):\", len(noise_custom))\n",
        "\n",
        "    print(\"Number of clusters (scikit-learn):\", len(clusters_sklearn))\n",
        "    print(\"Number of noise points (scikit-learn):\", len(noise_sklearn))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_dbscan()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ak1wcLs60GU",
        "outputId": "f78e3ddc-3e96-42d2-d17c-de51c5173f29"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of clusters (custom): 13\n",
            "Number of noise points (custom): 399\n",
            "Number of clusters (scikit-learn): 13\n",
            "Number of noise points (scikit-learn): 399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import DBSCAN as sklearn_DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import adjusted_rand_score\n",
        "\n",
        "\n",
        "def dbscan(data, columns, eps, min_samples):\n",
        "    \"\"\"\n",
        "    Perform DBSCAN clustering on the specified columns of a dataset.\n",
        "\n",
        "    Parameters:\n",
        "      data        : Iterable containing data instances.\n",
        "      columns     : List of column indices to use for clustering.\n",
        "      eps         : Radius to consider neighbors.\n",
        "      min_samples : Minimum number of neighbors to be a core point.\n",
        "\n",
        "    Returns:\n",
        "      List of clusters (each cluster is a list of data instances) and a list of noise points.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create a data subset based on specified columns for clustering\n",
        "    # Changed line: Use advanced indexing with columns list\n",
        "    D = np.array([instance for instance in data])\n",
        "    D = D[:, columns]\n",
        "    labels = [0] * len(D)  # Initialize all labels to 0 (unvisited)\n",
        "    C = 0  # Cluster ID\n",
        "\n",
        "    for P in range(len(D)):\n",
        "        if labels[P] != 0:  # Skip if the point has already been processed\n",
        "            continue\n",
        "\n",
        "        NeighborPts = region_query(D, P, eps)\n",
        "\n",
        "        # Check if the point is a core point\n",
        "        if len(NeighborPts) < min_samples:\n",
        "            labels[P] = -1  # Mark as noise\n",
        "        else:\n",
        "            C += 1  # Increment cluster ID\n",
        "            grow_cluster(D, labels, P, NeighborPts, C, eps, min_samples)\n",
        "\n",
        "    # Organize points into clusters and noise\n",
        "    clusters = [[] for _ in range(C)]\n",
        "    noise = []\n",
        "    for i, label in enumerate(labels):\n",
        "        if label == -1:\n",
        "            noise.append(data[i])\n",
        "        else:\n",
        "            clusters[label - 1].append(data[i])\n",
        "\n",
        "    return clusters, noise\n",
        "def grow_cluster(D, labels, P, NeighborPts, C, eps, min_samples):\n",
        "    \"\"\"\n",
        "    Grow a cluster from the seed point `P`.\n",
        "\n",
        "    Parameters:\n",
        "      D           : Data subset with relevant columns\n",
        "      labels      : List of point labels\n",
        "      P           : Seed point index\n",
        "      NeighborPts : List of neighbors of the seed point\n",
        "      C           : Cluster ID\n",
        "      eps         : Radius to consider neighbors\n",
        "      min_samples : Minimum number of neighbors to be a core point\n",
        "    \"\"\"\n",
        "    labels[P] = C  # Assign the cluster label to the seed point\n",
        "\n",
        "    i = 0\n",
        "    while i < len(NeighborPts):\n",
        "        Pn = NeighborPts[i]\n",
        "\n",
        "        if labels[Pn] == -1:\n",
        "            labels[Pn] = C  # Convert noise to border point\n",
        "\n",
        "        elif labels[Pn] == 0:\n",
        "            labels[Pn] = C\n",
        "            PnNeighborPts = region_query(D, Pn, eps)\n",
        "            if len(PnNeighborPts) >= min_samples:\n",
        "                NeighborPts += PnNeighborPts\n",
        "\n",
        "        i += 1\n",
        "\n",
        "\n",
        "def region_query(D, P, eps):\n",
        "    \"\"\"\n",
        "    Find neighbors within `eps` distance from point `P`.\n",
        "\n",
        "    Parameters:\n",
        "      D   : Data subset with relevant columns\n",
        "      P   : Index of the point in question\n",
        "      eps : Radius to consider neighbors\n",
        "\n",
        "    Returns:\n",
        "      List of indices of points within `eps` distance of `P`.\n",
        "    \"\"\"\n",
        "    neighbors = []\n",
        "    for Pn in range(len(D)):\n",
        "        if np.linalg.norm(D[P] - D[Pn]) < eps:\n",
        "            neighbors.append(Pn)\n",
        "\n",
        "    return neighbors\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test_dbscan():\n",
        "    \"\"\"\n",
        "    Tests the DBSCAN implementation against scikit-learn's DBSCAN.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load your dataset\n",
        "    data = pd.read_csv(\"combined_tracks_info_ArijitSingh_KK.csv\")  # Replace with your file name\n",
        "\n",
        "    # Select the relevant numerical columns for clustering\n",
        "    # You may need to adjust these columns based on your dataset.\n",
        "    columns_to_cluster = [\n",
        "        \"danceability\", \"energy\", \"loudness\", \"speechiness\", \"acousticness\",\n",
        "        \"instrumentalness\", \"liveness\", \"valence\", \"tempo\"\n",
        "    ]\n",
        "\n",
        "    X = data[columns_to_cluster].values\n",
        "\n",
        "    # Scale the data for better performance\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # DBSCAN parameters\n",
        "    eps = 0.8\n",
        "    min_samples = 7\n",
        "\n",
        "    # Run your DBSCAN implementation\n",
        "    clusters_custom, noise_custom = dbscan(\n",
        "        data=[list(row) for row in X_scaled],\n",
        "        columns=list(range(len(columns_to_cluster))),\n",
        "        eps=eps,\n",
        "        min_samples=min_samples\n",
        "    )\n",
        "\n",
        "    # Run scikit-learn's DBSCAN\n",
        "    dbscan_sklearn = sklearn_DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    labels_sklearn = dbscan_sklearn.fit_predict(X_scaled)\n",
        "\n",
        "    # Compare the results\n",
        "    clusters_sklearn = [[] for _ in range(len(set(labels_sklearn)) - (1 if -1 in labels_sklearn else 0))]\n",
        "    noise_sklearn = []\n",
        "    labels_custom = [-1] * len(X_scaled)\n",
        "    for i, cluster in enumerate(clusters_custom):\n",
        "        for point in cluster:\n",
        "          index = np.where((X_scaled == point).all(axis=1))[0][0]\n",
        "          labels_custom[index] = i\n",
        "\n",
        "    for i, label in enumerate(labels_sklearn):\n",
        "      if label == -1:\n",
        "        noise_sklearn.append(X_scaled[i])\n",
        "      else:\n",
        "        clusters_sklearn[label].append(X_scaled[i])\n",
        "\n",
        "\n",
        "    # Calculate Adjusted Rand Index (ARI) to evaluate cluster similarity\n",
        "    ari = adjusted_rand_score(labels_sklearn, labels_custom)\n",
        "    print(\"Adjusted Rand Index:\", ari)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_dbscan()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcaElvDy8i68",
        "outputId": "2d225a82-da6b-4829-f5ce-67367fd747ea"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusted Rand Index: 0.5798440646164023\n"
          ]
        }
      ]
    }
  ]
}